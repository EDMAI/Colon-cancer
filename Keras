from keras import backend as K
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.callbacks import EarlyStopping
import pandas as pd, numpy as np
from sklearn import model_selection

# train data import and nomalize
X = pd.read_csv("../data/train_keras.csv",encoding="MS949",thousands=',')
X = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
y_train=X.drop(X.columns[[range(1,127)]],axis=1).as_matrix()
X_train=X.drop(['TARGET'],1).as_matrix()

# test data import and nomalize
test = pd.read_csv("../data/test_keras.csv",encoding="MS949",thousands=',')
test = (test - test.min(axis=0)) / (test.max(axis=0) - test.min(axis=0))
y_test=test.drop(test.columns[[range(1,127)]],axis=1).as_matrix()
X_test=test.drop(['TARGET'],1).as_matrix()

# answer data import and nomalize
answer = pd.read_csv("../data/test_set_keras.csv",encoding="MS949", thousands=',')
answer = (answer - answer.min(axis=0)) / (answer.max(axis=0) - answer.min(axis=0))
a_test=answer.drop(['TARGET'],1).as_matrix()

print(y_train.shape)
print(X_train.shape)
print(y_test.shape)
print(X_test.shape)

# modeling
model = Sequential()
model.add(Dense(6000, input_shape=(126,))) # 1층
model.add(Activation('relu'))
model.add(Dropout(0.1))

model.add(Dense(256)) # 2층 
model.add(Activation('relu'))
model.add(Dropout(0.1))

model.add(Dense(1)) # 3층 
model.add(Activation('sigmoid'))


# def pred
def f1(y_true, y_pred):
    def recall(y_true, y_pred):
        """Recall metric.

        Only computes a batch-wise average of recall.

        Computes the recall, a metric for multi-label classification of
        how many relevant items are selected.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision(y_true, y_pred):
        """Precision metric.

        Only computes a batch-wise average of precision.

        Computes the precision, a metric for multi-label classification of
        how many selected items are relevant.
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision
    precision = precision(y_true, y_pred)
    recall = recall(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall))

# compile 
model.compile(loss='binary_crossentropy',
          optimizer= "adam",
          metrics=[f1])

# fitting a model
model.fit(X_train, y_train, epochs=1, batch_size=10000)

# evaluate a model
scores = model.evaluate(X_test, y_test)
print("%s: %.2f%%" %(model.metrics_names[1], scores[1]*100))
weights=model.layers[0].get_weights()[0]

print("weights=", weights)
biases = model.layers[0].get_weights()[1]
print("biases=", biases)
prediction = model.predict(a_test)

# savetext
pred=np.round(prediction, 3)
np.savetxt('prediction.txt', pred)
np.savetxt('weights.txt',weights)
np.savetxt('biases.txt', biases)
